NiFi
08:30 - 12:00
- About
--> is an easy to use, powerful, and reliable system to process and distribute data.
--> Put simply NiFi was built to automate the flow of data between systems. While the termdataflow is used in a variety of contexts, we’ll use it here to mean the automated and managed flow of information between systems. This problem space has been around ever since enterprises had more than one system, where some of the systems created data and some of the systems consumed data. The problems and solution patterns that emerged have been discussed and articulated extensively. A comprehensive and readily consumed form is found in theEnterprise Integration Patterns
- Systems fail
- Networks fail, disks fail, software crashes, people make mistakes.
- Data access exceeds capacity to consume
- Sometimes a given data source can outpace some part of the processing or delivery chain - it only takes one weak-link to have an issue.
- Boundary conditions are mere suggestions
- You will invariably get data that is too big, too small, too fast, too slow, corrupt, wrong, or in the wrong format.
- What is noise one day becomes signal the next
- Priorities of an organization change - rapidly. Enabling new flows and changing existing ones must be fast.
- Systems evolve at different rates
- The protocols and formats used by a given system can change anytime and often irrespective of the systems around them. Dataflow exists to connect what is essentially a massively distributed system of components that are loosely or not-at-all designed to work together.
- Compliance and security
- Laws, regulations, and policies change. Business to business agreements change. System to system and system to user interactions must be secure, trusted, accountable.
- Continuous improvement occurs in production
- It is often not possible to come even close to replicating production environments in the lab.
- Features
- Web-based user interface
- Seamless experience between design, control, feedback, and motem mediation logic.nitoring
- Highly configurable
- Loss tolerant vs guaranteed delivery
- Low latency vs high throughput
- Dynamic prioritization
- Flow can be modified at runtime
- Back pressure
- Data Provenance
- Track dataflow from beginning to end
- Designed for extension
- Build your own processors and more
- Enables rapid development and effective testing
- Secure
- SSL, SSH, HTTPS, encrypted content, etc...
- Pluggable role-based authentication/authorization
- Benefit
- Lends well to visual creation and management of directed graphs of processors
- Is inherently asynchronous which allows for very high throughput and natural buffering even as processing and flow rates fluctuate
- Provides a highly concurrent model without a developer having to worry about the typical complexities of concurrency
- Promotes the development of cohesive and loosely coupled components which can then be reused in other contexts and promotes testable units
- The resource constrained connections make critical functions such as back-pressure and pressure release very natural and intuitive
- Error handling becomes as natural as the happy-path rather than a coarse grained catch-all
- The points at which data enters and exits the system as well as how it flows through are well understood and easily tracked
- NiFi executes within a JVM living within a host operating system. The primary components of NiFi then living within the JVM are as follows
- Web Server
- The purpose of the web server is to host NiFi’s HTTP-based command and control API.
- Flow Controller
- The flow controller is the brains of the operation. It provides threads for extensions to run on and manages their schedule of when they’ll receive resources to execute.
- Extensions
- There are various types of extensions for NiFi which will be described in other documents. But the key point here is that extensions operate/execute within the JVM.
- FlowFile Repository
- The FlowFile Repository is where NiFi keeps track of the state of what it knows about a given FlowFile that is presently active in the flow. The implementation of the repository is pluggable. The default approach is a persistent Write-Ahead Log that lives on a specified disk partition.
- Content Repository
- The Content Repository is where the actual content bytes of a given FlowFile live. The implementation of the repository is pluggable. The default approach is a fairly simple mechanism, which stores blocks of data in the file system. More than one file system storage location can be specified so as to get different physical partitions engaged to reduce contention on any single volume.
- Provenance Repository
- The Provenance Repository is where all provenance event data is stored. The repository construct is pluggable with the default implementation being to use one or more physical disk volumes. Within each location event data is indexed and searchable.
- Cluster
--> A NiFi cluster is comprised of one or moreNiFi Nodes (Node) controlled by a single NiFi Cluster Manager (NCM). The design of clustering is a simple master/slave model where the NCM is the master and the Nodes are the slaves. The NCM’s reason for existence is to keep track of which Nodes are in the cluster, their status, and to replicate requests to modify or observe the flow. Fundamentally, then, the NCM keeps the state of the cluster consistent. While the model is that of master and slave, if the master dies the Nodes are all instructed to continue operating as they were to ensure the data flow remains live. The absence of the NCM simply means new nodes cannot join the cluster and cluster flow changes cannot occur until the NCM is restored.
- Characteristic
- For IO
- The throughput or latency one can expect to see will vary greatly on how the system is configured. Given that there are pluggable approaches to most of the major NiFi subsystems the performance will depend on the implementation. But, for something concrete and broadly applicable, let’s consider the out-of-the-box default implementations that are used. These are all persistent with guaranteed delivery and do so using local disk. So being conservative, assume roughly 50 MB/s read/write rate on modest disks or RAID volumes within a typical server. NiFi for a large class of dataflows then should be able to efficiently reach 100 or more MB/s of throughput. That is because linear growth is expected for each physical partition and content repository added to NiFi. This will bottleneck at some point on the FlowFile repository and provenance repository. We plan to provide a benchmarking/performance test template to include in the build, which will allow users to easily test their system and to identify where bottlenecks are and at which point they might become a factor. It should also make it easy for system administrators to make changes and to verify the impact.
- For CPU
- The Flow Controller acts as the engine dictating when a particular processor will be given a thread to execute. Processors should be written to return the thread as soon as they’re done executing their task. The Flow Controller can be given a configuration value indicating how many threads there should be for the various thread pools it maintains. The ideal number of threads to use will depend on the resources of the host system in terms of numbers of cores, whether that system is running other services as well, and the nature of the processing in the flow. For typical IO heavy flows though it would be quite reasonable to set many dozens of threads to be available if not more.
- For RAM
- NiFi lives within the JVM and is thus generally limited to the memory space it is afforded by the JVM. Garbage collection of the JVM becomes a very important factor to both restricting the total practical size the heap can be as well as how well the application will run over time.
- Key NiFi
- Guaranteed Delivery
- A core philosophy of NiFi has been that even at very high scale, guaranteed delivery is a must. This is achieved through effective use of a purpose-built persistent write-ahead log and content repository. Together they are designed in such a way as to allow for very high transaction rates, effective load-spreading, copy-on-write, and play to the strengths of traditional disk read/writes.
- Data Buffering w/ Back Pressure and Pressure Release
- NiFi supports buffering of all queued data as well as the ability to provide back pressure as those queues reach specified limits or to age off data as it reaches a specified age (its value has perished).
- Prioritized Queuing
- NiFi allows the setting of one or more prioritization schemes for how data is retrieved from a queue. The default is oldest first, but there are times when data should be pulled newest first, largest first, or some other custom scheme.
- Flow Specific QoS (latency v throughput, loss tolerance, etc.)
- There are points of a dataflow where the data is absolutely critical and it is loss intolerant. There are also times when it must be processed and delivered within seconds to be of any value. NiFi enables the fine-grained flow specific configuration of these concerns.
- Data Provenance
- NiFi automatically records, indexes, and makes available provenance data as objects flow through the system even across fan-in, fan-out, transformations, and more. This information becomes extremely critical in supporting compliance, troubleshooting, optimization, and other scenarios.
- Recovery / Recording a rolling buffer of fine-grained history
- NiFi’s content repository is designed to act as a rolling buffer of history. Data is removed only as it ages off the content repository or as space is needed. This combined with the data provenance capability makes for an incredibly useful basis to enable click-to-content, download of content, and replay, all at a specific point in an object’s lifecycle which can even span generations.
- Visual Command and Control
- Dataflows can become quite complex. Being able to visualize those flows and express them visually can help greatly to reduce that complexity and to identify areas that need to be simplified. NiFi enables not only the visual establishment of dataflows but it does so in real-time. Rather than being design and deploy it is much more like molding clay. If you make a change to the dataflow that change immediately takes effect. Changes are fine-grained and isolated to the affected components. You don’t need to stop an entire flow or set of flows just to make some specific modification.
- Flow Templates
- Dataflows tend to be highly pattern oriented and while there are often many different ways to solve a problem, it helps greatly to be able to share those best practices. Templates allow subject matter experts to build and publish their flow designs and for others to benefit and collaborate on them.
- Security
- System to system
- A dataflow is only as good as it is secure. NiFi at every point in a dataflow offers secure exchange through the use of protocols with encryption such as 2-way SSL. In addition NiFi enables the flow to encrypt and decrypt content and use shared-keys or other mechanisms on either side of the sender/recipient equation.
- User to system
- NiFi enables 2-Way SSL authentication and provides pluggable authorization so that it can properly control a user’s access and at particular levels (read-only, dataflow manager, admin). If a user enters a sensitive property like a password into the flow, it is immediately encrypted server side and never again exposed on the client side even in its encrypted form.
- Designed for Extension
- NiFi is at its core built for extension and as such it is a platform on which dataflow processes can execute and interact in a predictable and repeatable manner.
- Points of extension
- Processors, Controller Services, Reporting Tasks, Prioritizers, Customer User Interfaces
- Classloader Isolation
- For any component-based system, dependency nightmares can quickly occur. NiFi addresses this by providing a custom class loader model, ensuring that each extension bundle is exposed to a very limited set of dependencies. As a result, extensions can be built with little concern for whether they might conflict with another extension. The concept of these extension bundles is called NiFi Archives and will be discussed in greater detail in the developer’s guide.
- Link
Apache NiFi : https://nifi.apache.org
SolR
13:00 - 17:30
- About
--> is the popular, blazing-fast, open source enterprise search platform built on Apache Lucene
--> is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more. Solr powers the search and navigation features of many of the world's largest internet sites.
- Features
--> is a standalone enterprise search server with a REST-like API. You put documents in it (called "indexing") via JSON, XML, CSV or binary over HTTP. You query it via HTTP GET and receive JSON, XML, CSV or binary results.
- Advanced Full-Text Search Capabilities
- Powered by Lucene, Solr enables powerful matching capabilities including phrases, wildcards, joins, grouping and much more across any data type
- Optimized for High Volume Traffic
- Solr is proven at extremely large scales the world over
- Standards Based Open Interfaces [ XML, JSON and HTTP ]
- Solr uses the tools you use to make application building a snap
- Comprehensive Administration Interfaces
- Solr ships with a built-in, responsive administrative user interface to make it easy to control your Solr instances
- Easy Monitoring
- Need more insight into your instances? Solr publishes loads of metric data via JMX
- Highly Scalable and Fault Tolerant
- Built on the battle-tested Apache Zookeeper, Solr makes it easy to scale up and down. Solr bakes in replication, distribution, rebalancing and fault tolerance out of the box.
- Flexible and Adaptable with easy configuration
- Solr's is designed to adapt to your needs all while simplifying configuration
- Near Real-Time Indexing
- Want to see your updates now? Solr takes advantage of Lucene's Near Real-Time Indexing capabilities to make sure you see your content when you want to see it
- Extensible Plugin Architecture
- Solr publishes many well-defined extension points that make it easy to plugin both index and query time plugins. Of course, since it is Apache-licensed open source, you can change any code you want!
- Schema when you want, schemaless when you don't
- Use Solr's data-driven schemaless mode when getting started and then lock it down when it's time for production.
- Powerful Extensions
- Solr ships with optional plugins for indexing rich content (e.g. PDFs, Word), language detection, search results clustering and more
- Faceted Search and Filtering
- Slice and dice your data as you see fit using a large array of faceting algorithms
- Geospatial Search
- Enabling location-based search is simple with Solr's built-in support for spatial search
- Advanced Configurable Text Analysis
- Solr ships with support for most of the widely spoken languages in the world (English, Chinese, Japanese, German, French and many more) and many other analysis tools designed to make indexing and querying your content as flexible as possible
- Highly Configurable and User Extensible Caching
- Fine-grained controls on Solr's built-in caches make it easy to optimize performance
- Performance Optimizations
- Solr has been tuned to handle the world's largest sites
- Security built right in
- Secure Solr with SSL, Authentication and Role based Authorization. Pluggable, of course!
- Advanced Storage Options
- Building on Lucene's advanced storage capabilities (codecs, directories and more), Solr makes it easy to tune your data storage needs to fit your application
- Monitorable Logging
- Easily access Solr's log files from the admin interface
- Query Suggestions, Spelling and More
- Solr ships with advanced capabilites for auto-complete (typeahead search), spell checking and more
- Your Data, Your Way!
- JSON, CSV, XML and more are supported out of the box. Don't waste precious time converting all your data to a common representation, just send it to Solr!
- Rich Document Parsing
- Solr ships with Apache Tika built-in, making it easy to index rich content such as Adobe PDF, Microsoft Word and more.
- Apache UIMA
- Ready to enhance your content with advanced annotation engines? Solr integrates into Apache UIMA, making it easy to leverage NLP and other tools as part of your application.
- Multiple search indices
- Solr supports multi-tenant architectures, making it easy to isolate users and content.
- Data Handling
--> Schema or schemaless, easily define the field types, analysis processes and document structures to make your search application successful
- Schemaless (data-driven schema) makes it easy to get started, while switching to a configured schema makes for a solid production environment.
- Solr supports both schemaless and schema modes, depending on your goals
- Solr's Field Types make it easy to declaratively mix and match Lucene Analyzers without writing code
- char filters edit the text prior to tokenization
- tokenizers break the text up into terms
- token filters transform the terms
- Dynamic Fields enable on-the-fly addition of new fields that auto-map to field types based on the field name
- Easily analyze the same content in different ways using Solr's copy field capabilities
- Explicit types eliminate the need for guessing types of fields and introducing noise
- External file-based configuration of stopword lists, synonym lists, and protected word lists
- Many additional text analysis components including word splitting, regex, stemming and more
- Extensible to allow for applications to easily handle proprietary formats and unsupported data types
- Query
--> HTTP interfaces with flexible IO formats and extensive query parsing support make adding and finding data a snap
- REST interfaces provide for easy integration with any language
- Native clients are also available
- Sort by any number of fields, and by complex functions of numeric fields
- Documents with missing values can be configured to sort last (or first)
- Function queries provide powerful relevance tuning capabilities based off of your data
- Advanced relevance tuning options enable applications to fine tune results to fit their demanding needs
- Solr supports multiple approaches to query parsing, making it easy to find your data
- Dozens of query types (boolean, phrase, term, numeric, fielded and more) provide incredible power when searching
- Good out of the box defaults make getting great results easy, while extensive configuration options make it easy to fine tune
- Request handling can easily be configured to provide server-side control and simplification of how applications request results
- Extensive filtering features allow applications to control what content is searched and when
- Multiple scoring (similarity) approaches allow for easy experimentation and relevance tuning
- Pluggable query parsers mean every application can provide a query interface that makes sense to its users
- Well-defined APIs for capturing document and collection level statistics enable custom similarity models with little effort
- Cursors (aka "deep paging") enables next generation NoSQL data storage and analysis options
- Near Real Time (NRT) search allows access to document addition and updates almost immediately
- Facets
--> Slice and dice your data six ways from sunday
- Powerful faceting features support grouping and organizing your data in a multitude of ways
- Pivot faceting enables asking sophisticated "what if" questions
- Fine-grained performance controls make it possible to tradeoff competing CPU and memory resources during faceting
- Term, Query, Range, Date and Pivot faceting implementations make data investigation simple and powerful
- Range faceting enables grouping time and numerical content in easy to understand buckets
- Query-based faceting makes it easy to facet by arbitrary queries
- Multi-select facet parameters enable exact control over how facets interact with the original result set
- Discovery
--> Clustering, spellchecking, auto-complete and more make it easy to help users discover content
- Typeahead, autocomplete, auto-suggest, whatever you call it, Solr supports powerful and extensible mechanisms for providing suggestions to users as they type in their queries
- Integrated Carrot2 features enable dynamic search results clustering
- Spelling suggestions are a breeze to enable to help users correct their queries for better results
- Configurable hit highlighting helps users focus in on exactly where matches occur
- Plugins and Extension
--> Open source and well-documented make Solr highly extensible
- Plugins for language detection, clustering, data-import, UIMA, Apache Velocity and more provide optional functionality when needed
- Well-defined extension points for indexing, analysis, request handling, query parsing and more make focused extensions easy to write and enable
- An open source and open development approach, enabled by the Apache Software Foundation, make contributions to Solr easy to do
- Don't like how Solr implements something? Found a bug? Check out the code from SVN or Git and make changes as you see fit. Patches welcome!
- Statistics and Aggregation
- Solr supports complex mathematical analysis on result sets, enabling next-generation search-based business intelligence
- Grouping, nesting and other approaches enable applications to represent complex relationships between content types
- Function query results can be returned with hits, and used to sort or boost hits
- Field Statistics can be combined with nested Pivot Faceting
- The stats component can compute min, max, sum, mean, distinct values, and more
- Spatial
--> Location-aware search using advanced spatial integrations
- Out of the box support for location-aware search (think mobile search) enables applications to find and filter content based on location
- Extensive support for representing complex spatial data like polygons, lat/lon and more
- Advanced query capabilities allow for bounding box, polygonal and other complex querying capabilities
- Integration with the Java Topological Suite library provides advanced spatial capabilities
- Rich Content
--> Your Data, your way. PDF, Word, Powerpoint and more
- Integration with Apahe Tika enables the extraction and processing of rich content types without writing a single line of code
- Data not in JSON? No problem. Solr handles a variety of data types out of the box, including JSON, XML, many Office documents, CSV and more
- Flexible and extensible Input and Output formats allow your application to talk with Solr the way you want to talk to it
- Advanced processing options make it easy to control what extracted content is indexed and how it is indexed
- Performance
--> Proven time and time again in world-class organizations, Solr performs and scales to handle the most demanding applications
- Well tested defaults provide good performance out of the box with little tuning
- Advanced options give fine grained control over numerous performance aspects
- Powerful single-node and distributed capabilities simplify the process of going from one node to many
- A multitude of smart caching options enable exacting control over repetitive results a breeze
- Always built on the latest and greatest Lucene, Solr takes advantage of numerous Lucene performance enhancements
- Support for codecs, alternate Directory implementations and more provide for advanced performance tuning options
- Scaling
--> Highly scalable, fault tolerant distributedc infrastructure built on Apache Zookeeper
- Add and remove compute capacity as needed, while letting Solr do the dirty work of load balancing and more
- Shard splitting makes it easy to incrementally add capacity without re-indexing your entire content
- Built on tried and true Apache Zookeeper, Solr favors consistency and avoids hairy split brain issues common to other approaches
- Built-in User Interfaces provide deep insight into cluster state
- Advanced transaction log, replication and failover capabilities help minimize data loss while still serving results
- Tested regularly at scale to handle hundreds to thousands of compute nodes and billions of documents with very high query volumes
- Admin Interface
--> A rich and powerful User Interface enables deep insight into Solr's state all without anything extra to install
- Extensive reporting and control interfaces make it easy to understand and alter what is happening in Solr
- Modern, responsive Javascript-based implementation provides instant feedback
- Gain insight into cores, collections, shards and more
- Explore logs, threads and system properties without ever leaving the browser
- Want to explore alternate queries? Try the query interface and instantly explore your data without having to write code
Explore and debug your data and queries with advanced interfaces for analysis, data exploration and more
- Index content directly from the browser, including rich content via file uploading
- Instantly review key performance metrics right in the browser
- Link
Apache SolR : http://lucene.apache.org/solr/
