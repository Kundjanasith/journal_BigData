Machine Learning
08:30 - 12:00
- Introduction to Machine Learning
- explores the study and construction of algorithm that can learn from and make predictions on data.
- algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs.
- a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field.
- is employed in a range of computing tasks where designing and programming explicit algorithms is unfeasible.
- is sometimes conflated with data mining, where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning.
- is a method used to devise complex models and algorithms that lend themselves to prediction - in commercial use, this is known as predictive analytic. These analytical models allow researchers, data sciences, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.
- Quote
- A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E
- Problems // Tasks
--> are typically classified into three broad categories, depending on the nature of the learning "signal" or "feedback" available to a learning system.
- Supervised Learning
- The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.
- Unsupervised Learning
 - No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).
- Reinforcement Learning
- A computer program interacts with a dynamic environment in which it must perform a certain goal, without a teacher explicitly telling it whether it has come close to its goal. Another example is learning to play a game by playing against an opponent
- Semi-supervised Learning
- between supervised learning and unsupervised learning
- the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing
- Transduction
- is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing.
- Developmental Learning
- learning to learn learns its own inductive bias based on previous experience
- elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.
- Categorization of machine learning tasks arises when one considers the desired output of a machine-learned system
- In Classification
- inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes.
- is typically tackled in a supervised way. Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are "spam" and "not spam".
- In Regression
- a supervised problem, the outputs are continuous rather than discrete.
- In Clustering
- a set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.
--> Density estimation finds the distribution of inputs in some space.
--> Dimensionality reduction simplifies inputs by mapping them into a lower-dimensional space.
-->  Topic modeling is a related problem, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.
- Algorithm
- Decision tree learning
--> uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value.
- Decision tree
- is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.
- It is one way to display an algorithm.
- Predictive model
- uses statistics to predict outcomes.
- often the event one wants to predict is in the future, but predictive modeling can be applied to any type of unknown event, regardless of when it occurred.
- often used to detect crimes and identify suspects, after the crime has taken place
- Association rule learning
--> Association rule learning is a method for discovering interesting relations between variables in large databases.
- Artificial neural networks
--> usually called "neural network" (NN), is a learning algorithm that is inspired by the structure and functional aspects of biological neural network.
- Biological network
- is a series of interconnected neurons whose activation defines a recognizable linear pathway.
--> Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionism approach to computation.
- Artificial neurons
- is a mathematical function conceived as a model of biological neurons.
- Artificial neurons are the constitutive units in an artificial neuron netowrk. Depending on the specific model used they may be called a semi-linear unit, Nv neuron, binary neuron, linear threshold function, or McCullochâ€“Pitts (MCP) neuron
- The artificial neuron receives one or more inputs (representing dendrites) and sums them to produce an output (representing a neuron's axon).
- Connectionism
- is a set of approaches in the fields of artificial intelligence, cognitive psychology, cognitive science, neuroscience, and philosophy of mind, that models mental or behavioral phenomena as the emergent processes of interconnected networks of simple units.
- Computational
- is any type of calculation that follows a well-defined model understood and expressed as, for example, an algorithm.
--> Modern neural networks are non-linear statistical data mining tools\
-->  They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown  joint probability distribution between observed variables.
- Deep learning
--> Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of Deep learning which consists of multiple hidden layers in an artificial neural network
--> This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition
- Inductive logic programming
--> Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses.
--> Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples
- Entailment
 - is logical consequence which one of the most fundamental concepts in logic. It is the relationship between statements that holds true when one logically "follows from" one or more others
--> Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.
- Support vector machine
--> Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression.
--> Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.
- Clustering
--> Cluster analysis is the assignment of a set of observations into subsets (calledclusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar.
--> Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters.
--> Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.
- Bayesian networks
--> A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variable and theircondiional independent via a directed acyclic grpah (DAG).
- Coordinational independencies
- For example
- two events R and B are conditionally independent given a third event Y precisely if the occurrence or non-occurrence of Rand the occurrence or non-occurrence of B are independent events in their conditional probability distribution given Y.
- Directed acyclic graph
- s a finite directed graph with no directred cycle. That is, it consists of finitely many vertices and edges, with each edge directed from one vertex to another, such that there is no way to start at any vertex v and follow a consistently-directed sequence of edges that eventually loops back to v again.
- For example
- Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.
- Reinforcement learning
--> Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize
some notion of long-term reward.
--> learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states.
--> learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.
- Representation learning
--> Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training.
--> Classical examples include principal components analysis and cluster analysis.
- principal components analysis
-  is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables
- cluster analysis
- s the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).
--> Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing to reconstruct the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.
- Similarity and metric learning
--> the learning machine is given pairs of examples that are considered similar and pairs of less similar objects.
--> It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar.
--> It is sometimes used in Recommendation systems
- Recommendation systems
- are a subclass of information filtering system that seek to predict the 'rating' or 'preference' that a user would give to an item
- Sparse dictionary learning
--> a datum is represented as a linear combination of basis functions, and the coefficients are assumed to be sparse.
--> Let x be a d-dimensional datum, Dbe a d by n matrix, where each column of D represents a basis function. r is the coefficient to represent x using D.
--> Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to
--> Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary.
--> Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.
- Genetic algorithm
--> A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem.

Spark MLlib - Scala
13:00 - 17:30
- MLlib is spark's machine learning library
- Its goal is to make practical machine learning scalable and easy.
- It consists of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as lower-level optimization primitives and higher-level pipeline APIs.
- Packages
- spark .mllib
--> contains the original API built on top of RDDs
- spark.ml
--> provides higher-level API built on top of DataFrames  for constructing ML pipelines.
- spark.mllib : Data types, Algorithm, Utilities
- Data types
- Local vector
import org.apache.spark.mllib.linalg.{Vector, Vectors}
val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)
val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))
val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))
- Labeled point
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
val pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))
val neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))
==============================================================
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.rdd.RDD
val examples: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
- Local matrix
import org.apache.spark.mllib.linalg.{Matrix, Matrices}
val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))
val sm: Matrix = Matrices.sparse(3, 2, Array(0, 1, 3), Array(0, 2, 1), Array(9, 6, 8))
- Distributed matrix
- Row Matrix
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.linalg.distributed.RowMatrix
val rows: RDD[Vector] = ... // an RDD of local vectors
val mat: RowMatrix = new RowMatrix(rows)
val m = mat.numRows()
val n = mat.numCols()
val qrResult = mat.tallSkinnyQR(true)
- Indexed Row Matrix
import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix, RowMatrix}
val rows: RDD[IndexedRow] = ... // an RDD of indexed rows
val mat: IndexedRowMatrix = new IndexedRowMatrix(rows)
val m = mat.numRows()
val n = mat.numCols()
val rowMat: RowMatrix = mat.toRowMatrix()
- Coordinate Matrix
import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}
val entries: RDD[MatrixEntry] = ... // an RDD of matrix entries
val mat: CoordinateMatrix = new CoordinateMatrix(entries)
val m = mat.numRows()
val n = mat.numCols()
val indexedRowMatrix = mat.toIndexedRowMatrix()
- Block Matrix
import org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}
val entries: RDD[MatrixEntry] = ...
val coordMat: CoordinateMatrix = new CoordinateMatrix(entries)
val matA: BlockMatrix = coordMat.toBlockMatrix().cache()
matA.validate()
val ata = matA.transpose.multiply(matA)
- Basic statistic
- Summary statistics
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
val observations: RDD[Vector] = ... // an RDD of Vectors
val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)
println(summary.mean)
println(summary.variance)
println(summary.numNonzeros)
- Correlations
import org.apache.spark.SparkContext
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.stat.Statistics
val sc: SparkContext = ...
val seriesX: RDD[Double] = ...
val seriesY: RDD[Double] = ...
val correlation: Double = Statistics.corr(seriesX, seriesY, "pearson")
val data: RDD[Vector] = ...
val correlMatrix: Matrix = Statistics.corr(data, "pearson")
- Stratified sampling
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.rdd.PairRDDFunctions
val sc: SparkContext = ...
val data = ...
val fractions: Map[K, Double] = ...
val approxSample = data.sampleByKey(withReplacement = false, fractions)
val exactSample = data.sampleByKeyExact(withReplacement = false, fractions)
- Hypothesis testing
import org.apache.spark.SparkContext
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.stat.Statistics._
val sc: SparkContext = ...
val vec: Vector = ...
val goodnessOfFitTestResult = Statistics.chiSqTest(vec)
println(goodnessOfFitTestResult)
val mat: Matrix = ...
val independenceTestResult = Statistics.chiSqTest(mat)
println(independenceTestResult)
val obs: RDD[LabeledPoint] = ...
val featureTestResults: Array[ChiSqTestResult] = Statistics.chiSqTest(obs)
var i = 1
featureTestResults.foreach { result =>
    println(s"Column $i:\n$result")
    i += 1
} // summary of the test
========================
import org.apache.spark.mllib.stat.Statistics
val data: RDD[Double] = ...
val testResult = Statistics.kolmogorovSmirnovTest(data, "norm", 0, 1)
println(testResult)
val myCDF: Double => Double = ...
val testResult2 = Statistics.kolmogorovSmirnovTest(data, myCDF)
- Streaming significance testing
val data = ssc.textFileStream(dataDir).map(line => line.split(",") match {
case Array(label, value) => BinarySample(label.toBoolean, value.toDouble) })
val streamingTest = new StreamingTest()
  .setPeacePeriod(0)
  .setWindowSize(0)
  .setTestMethod("welch")
val out = streamingTest.registerStream(data)
out.print()
- Random data generation
import org.apache.spark.SparkContext
import org.apache.spark.mllib.random.RandomRDDs._
val sc: SparkContext = ...
val u = normalRDD(sc, 1000000L, 10)
val v = u.map(x => 1.0 + 2.0 * x)
- Kernel Density Estination
import org.apache.spark.mllib.stat.KernelDensity
import org.apache.spark.rdd.RDD
val data: RDD[Double] = ...
val kd = new KernelDensity()
  .setSample(data)
  .setBandwidth(3.0)
val densities = kd.estimate(Array(-1.0, 2.0, 5.0))
- Classification and regression
- Linear models
- Support Vector Machine
- Logistic regression
- Linear regression
- Naive Bayes
- Decision tress
- Ensembles of tress
- Random forests
- Gradient-Boosted trees
- Isotonic regression
- Collaborative filtering
- Alternating Least Squares
- Clustering
- K-Means
- Gaussian mixture
- Power Iteration Clustering
- Latent Dirichlet Allocation
- Bisecting K-Means
- Streaming K-Means
- Dimensionality reduction
- Singular value clustering
- Principal component analysis
- Feature extraction and transformation
- Frequent pattern minning
- FP-growth
- Association rules
- PrefixSpan
- Evaluation metrics
- PMML mode export
- Optimization [ Developer ]
- Stochastic gradient descent
- Limited-memory BFGS
- spark.ml : High-level APIs for ML pipelines
- Overview : estimators, transformers and pipelines
- Extracting, Transforming and selecting features
- Classification and regression
- Clustering
