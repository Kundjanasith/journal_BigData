Machine Learning using Microsoft Azure
08:30 - 12:00
- We will seperate into 2 model
- Dpeth Model
- Predict : Depth group --> Answer will be 3 classification with probability
- Magnitude Model
- Predict : Magnitude group --> Answer will be 5 classification with probability
- Depth Model
- Import Data --> Earthquake Data
- 628,884 Rows
- 12 columns
- Time
- Latitude
- Longitude
- Depth
- Magnitude
- Depth_group
- Magnitude_group
- Continent
- Country
- Day
- Month
- Year
- Select columns in Data Set
- Include 8 columns
- Latitude
- Longitude
- Depth_group
- Continent
- Country
- Day
- Month
- Year
- Exclude 4 columns
- Time
- Depth
- Magnitude
- Magnitude_group
- Edit Metadata
- Make categorical
- Depth_group
- Day
- Month
- Year
- Clean missing data
- Split data
- Train Data --> 0.8 --> 80%
- Test Data --> 0.2 --> 20%
- Algorithm
- Multiclass - Decision Forest
- Train model
- Train Data <-- Split data
- Algorithm <-- Multiclass - Decision Forest
- Score Model
- Reslt from train model
- Test data <-- Split Data
- Evaluate Model
- Magnitude Model
- Import Data --> Earthquake Data
- 628,884 Rows
- 12 columns
- Time
- Latitude
- Longitude
- Depth
- Magnitude
- Depth_group
- Magnitude_group
- Continent
- Country
- Day
- Month
- Year
- Select columns in Data Set
- Include 10 columns
- Latitude
- Longitude
- Depth
- Depth_group
- Magnitude_group
- Continent
- Country
- Day
- Month
- Year
- Exclude 2 columns
- Time
- Magnitude
- Edit Metadata
- Make categorical
- Magnitude_group
- Depth_group
- Day
- Month
- Year
- Clean missing data
- Split data
- Train Data --> 0.8 --> 80%
- Test Data --> 0.2 --> 20%
- Algorithm
- Multiclass - Decision Forest
- Train model
- Train Data <-- Split data
- Algorithm <-- Multiclass - Decision Forest
- Score Model
- Reslt from train model
- Test data <-- Split Data
- Evaluate Model
- Result from Depth Model
- Probability Earthquake in 1 Depth_group
- Probability Earthquake in 2 Depth_group
- Probability Earthquake in 3 Depth_group
- Result from Magnitude Model
- Probability Earthquake in 1 Magnitude_group
- Probability Earthquake in 2 Magnitude_group
- Probability Earthquake in 3 Magnitude_group
- Probability Earthquake in 4 Magnitude_group
- Probability Earthquake in 5 Magnitude_group
Machine Learning using Spark MLlib
13:00 - 17:30
- We will seperate into 2 model
- Dpeth Model
- Predict : Depth group --> Answer will be 3 classification with probability
- Magnitude Model
- Predict : Magnitude group --> Answer will be 5 classification with probabilit
- Algorithm --> Random Forest
- Random forests are one of the most successful machine learning models for classification and regression.
- They combine many decision trees in order to reduce the risk of overfitting.
- Like decision trees, random forests handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions.
--> Overview
- Random forests train a set of decision trees separately, so the training can be done in parallel.
- The algorithm injects randomness into the training process so that each decision tree is a bit different.
- Combining the predictions from each tree reduces the variance of the predictions, improving the performance on test data.
--> Training
- The randomness injected into the training process includes:
- Subsampling the original dataset on each iteration to get a different training set.
- Considering different random subsets of features to split on at each tree node.
- Apart from these randomizations, decision tree training is done in the same way as for individual decision trees.
--> Prediction
- To make a prediction on a new instance, a random forest must aggregate the predictions from its set of decision trees.
- Aggregation
- Classification
- Majority vote. Each tree’s prediction is counted as a vote for one class.
- The label is predicted to be the class which receives the most votes.
- Regression
- Averaging. Each tree predicts a real value.
- The label is predicted to be the average of the tree predictions.
--> Parameter
- numTrees
--> Number of trees in the forest
- Increasing the number of trees will decrease the variance in predictions, improving the model’s test-time accuracy.
- Training time increases roughly linearly in the number of trees.
- maxDepth
--> Maximum depth of each tree in the forest
- Increasing the depth makes the model more expressive and powerful. However, deep trees take longer to train and are also more prone to overfitting.
- In general, it is acceptable to train deeper trees when using random forests than when using a single decision tree. One tree is more likely to overfit than a random forest
- subsamplingRate
- This parameter specifies the size of the dataset used for training each tree in the forest, as a fraction of the size of the original dataset. The default (1.0) is recommended, but decreasing this fraction can speed up training.
- featureSubsetStartegy
- Number of features to use as candidates for splitting at each tree node. The number is specified as a fraction or function of the total number of features. Decreasing this number will speed up training, but can sometimes impact performance if too low.
--> Code --> Classification
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.util.MLUtils
val data = MLUtils.loadLibSVMFile(sc, "svmfile.txt")
val splits = data.randomSplit(Array(0.8, 0.2))
val (trainingData, testData) = (splits(0), splits(1))
val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val numTrees = 3 // Use more in practice.
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val impurity = "gini"
val maxDepth = 4
val maxBins = 32
val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)
val labelAndPreds = testData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()
println("Test Error = " + testErr)
println("Learned classification forest model:\n" + model.toDebugString)
model.save(sc, "hdfs:path")
val sameModel = RandomForestModel.load(sc, "hdfs:patModel")
--> Code --> Regression
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.util.MLUtils
val data = MLUtils.loadLibSVMFile(sc, "svmfile.tx.txt")
val splits = data.randomSplit(Array(0.7, 0.3))
val (trainingData, testData) = (splits(0), splits(1))
val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val numTrees = 3 // Use more in practice.
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val impurity = "variance"
val maxDepth = 4
val maxBins = 32
val model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo,
  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)
val labelsAndPredictions = testData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()
println("Test Mean Squared Error = " + testMSE)
println("Learned regression forest model:\n" + model.toDebugString)
model.save(sc, "hdfs:patMododel")
val sameModel = RandomForestModel.load(sc, "hdfs:patMododel")
- Random Forest
-  is a notion of the general technique of random decision forest that are an ensemble learnning method foclassification, regression and other tasks, that operate by constructing a multitude of decision tress at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.
- To measure the importance of the X feature after training, the values of the X feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set.
- The importance score for the X feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees.
- The score is normalized by the standard deviation of these differences
- Features which produce large values for this score are ranked as more important than features which produce small values.
- Spark ML
- Following by Machine Learning using Microsoft Azure
- Multilayer Perceptron Classification --> Multiclass Nueral Network
- Multilayer perceptron classifier (MLPC) is a classifier based on the feedforward artificial neural network. MLPC consists of multiple layers of nodes.
- Each layer is fully connected to the next layer in the network
- Nodes in the input layer represent the input data
- All other nodes maps inputs to the outputs by performing linear combination of the inputs with the node’s weights w and bias b and applying an activation function.
- MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.
- Code
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val data = sqlContext.read.format("libsvm")
  .load("data/mllib/sample_multiclass_classification_data.txt")
val splits = data.randomSplit(Array(0.8, 0.2), seed = 1234L)
val train = splits(0)
val test = splits(1)
val layers = Array[Int](4, 5, 4, 3)
val trainer = new MultilayerPerceptronClassifier()
  .setLayers(layers)
  .setBlockSize(128)
  .setSeed(1234L)
  .setMaxIter(100)
val model = trainer.fit(train)
val result = model.transform(test)
val predictionAndLabels = result.select("prediction", "label")
val evaluator = new MulticlassClassificationEvaluator()
  .setMetricName("precision")
println("Precision:" + evaluator.evaluate(predictionAndLabels))
- Classification
- Logistic regression
- Decision tree classifier
- Random forest classifier
- Gradient-boosted tree classifier
- Multilayer perceptron classifier
- One-vs-Rest classifier
- Regression
- Linear regression
- Decision tree regression
- Random forest regression
- Gradient-boosted tree regression
- Survival regression
- Decision trees
- Tree ensembles
- Random Forest
- Gradient-Boosted Trees
