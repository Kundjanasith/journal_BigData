Flume Laboratory
08:30 - 12:00
- ANALYZING SOCIAL MEDIA AND CUSTOMER SENTIMENT
[ ANALYZE TWITTER DATA WITH APACHE NIFI AND HDP SEARCH ]
- Technologies
- Hortonworks Sandbox
- Apache NiFi
- Solr + LucidWorks HDP Search
- Hive and Ambari Views
- Apache Zeppelin
- Twitter API
Host Database
localhost is used to configure the loopback interface
when the system is booting. Do not change this entry.
- 127.0.0.1 --> localhost
- 255.255.255.255 --> broadcasthost
- ::1 --> localhost
- 127.0.0.1 --> sandbox.hortonworks.com
- Step
- Install Apache NiFi
- Download Apache NiFi --> wget
- unzip file --> tar, unzip
- create variable --> export variable_name = variable_value
- edit nifi.sh --> nifi.web.http.port = 8080
- execute nifi.sh --> nifi.sh start
- Configure and Start Solr
- Download Apache Solr --> wget
- unzip file --> tar, unzip
- create variable --> export variable_name = variable_value
- solr on port --> solr start -e cloud -noprompt
- port --> 8983
- port --> 7574
- Create a Twitter Application
- Create a new Twitter App
- Application Details
- Name
- Description
- Website
- Application Permissions
- Read Only
- Application Settings
- Customer Key ( API Key )
- Customer Secret ( API Secret )
- Applicatiion Keys and Access Tokens
- Access Token
- Access Token Secret
- Create a Data Flow with NiFi
- Download Twitter Dashboard
- Browse the Twitter Dashboard [ Twitter_Dashboard.xml ]
- Import --> Twitter_Dashboard.xml
- Create Template --> Instantiate Template
- Each box --> Configure
- Configure Processor --> Properties
- Customer Key ( API Key )
- Customer Secret ( API Secret )
- Access Token
- Access Token Secret
- Generating Random Twitter Data For Hive & SolR
- wget https://raw.githubusercontent.com/hortonworks/tutorials/
hdp/assets/nifi-sentiment-analytics/assets/twitter-gen.sh
- bash twitter-gen.sh {NUMBER_OF_TWEETS}
- set data directory
- Analyze and Search Data with Solr
- create core --> "Tweet_Shard"
- Query data from data in data directory
- Execute query
- Analyze Tweet Data in Hive
- Start/Stop Data in Apache NiFi
- sudo -u hdfs hadoop fs -chown -R azure /tmp/tweets_staging
- sudo -u hdfs hadoop fs -chmod -R 777 /tmp/tweets_staging
- login credentials
--> Virtual Box
- Username --> maria_dev
- Password --> maria_dev
--> Microsoft Azure
- Username --> azure
- Password --> azure
- Create a table for the tweets
- CREATE EXTERNAL TABLE IF NOT EXISTS tweets_text(
  tweet_id bigint,
  created_unixtime bigint,
  created_time string,
  lang string,
  displayname string,
  time_zone string,
  msg string,
  fulltext string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY "|"
LOCATION "/tmp/tweets_staging";
- sudo -u hdfs hadoop fs -chmod -R 777 /tmp/data/tables
- CREATE EXTERNAL TABLE if not exists dictionary (
     type string,
     length int,
     word string,
     pos string,
     stemmed string,
     polarity string )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION '/tmp/data/tables/dictionary';
- CREATE EXTERNAL TABLE if not exists time_zone_map (
    time_zone string,
    country string,
    notes string )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION '/tmp/data/tables/time_zone_map';
- Create view
- CREATE VIEW IF NOT EXISTS tweets_simple AS
SELECT
tweet_id,
cast ( from_unixtime( unix_timestamp(concat( '2015',substring(created_time,5,15)), 'yyyy MMM dd hh:mm:ss')) as timestamp) ts,
msg,
time_zone
FROM tweets_text;
- CREATE VIEW IF NOT EXISTS tweets_clean AS
SELECT
t.tweet_id,
t.ts,
t.msg,
m.country
FROM tweets_simple t LEFT OUTER JOIN time_zone_map m ON t.time_zone = m.time_zone;
- create view IF NOT EXISTS l1 as select tweet_id, words from tweets_text lateral view explode(sentences(lower(msg))) dummy as words;
- create view IF NOT EXISTS l2 as select tweet_id, word from l1 lateral view explode( words ) dummy as word;
- create view IF NOT EXISTS l3 as select
tweet_id,
l2.word,
case d.polarity
when  'negative' then -1
when 'positive' then 1
else 0 end as polarity
 from l2 left outer join dictionary d on l2.word = d.word;
- create table IF NOT EXISTS tweets_sentiment stored as orc as select
tweet_id,
case
when sum( polarity ) > 0 then 'positive'
when sum( polarity ) < 0 then 'negative'
else 'neutral' end as sentiment
from l3 group by tweet_id;
- CREATE TABLE IF NOT EXISTS tweetsbi
STORED AS ORC
AS SELECT t.*,
case s.sentiment
when 'positive' then 2
when 'neutral' then 1
when 'negative' then 0
end as sentiment
FROM tweets_clean t LEFT OUTER JOIN tweets_sentiment s on t.tweet_id = s.tweet_id;
- Visualize Sentiment with Zeppelin
- port 9995 --> port Zappelin
- Hive --> Execute Query command
- Hands - On : Loading Twitter Data to Hadoop HDFS
- Exercise Overview
- Custome Flume Source
Twitter <--> Flume
- Sink to Hadoop File System
Flume --> Hadoop File System
- JSON SerDe Parse Data
Hadoop Fil System <-- Hive
- Step
1. Installing Flume
- Microsoft Azure --> Flume Service [ Already ]
2. Installing a jar file
- wget http://files.cloudera.com/samples/flume-sources-1.0-SNAPSHOT.jar
- sudo mv flume-sources-1.0-SNAPSHOT.jar /usr/local/flume/lib/
- cd /usr/local/flume/conf/
- sudo cp flume-env.sh.template flume-env.sh
- sudo vi flume-env.sh
3. Create a new Twitter App
- Login to your Twitter @ twitter.com
- Create a new Twitter App @ apps.twitter.com
- Enter all the details in the application
- Name
- Description
- Website
- Click on Keys and Access Tokens
- Customer Key ( API Key )
- Customer Secret ( API Secret )
- Access Token
- Access Token Secret
4. Configuring the Flume Agent Copy
- git clone https://github.com/cloudera/cdh-twitterexample
- vi /usr/local/flume/conf/flume.conf
- TwitterAgent.sources.Twitter.customerKey
- TwitterAgent.sources.Twitter.customerSecret
- TwitterAgent.sources.Twitter.accesToken
- TwitterAgent.sources.Twitter.accessTokenSecret
- TwitterAgent.sinks.HDFS.hdfs.path
5. Fetching the data from twitter
- flume-ng agent -n TwitterAgent -c conf -f /usr/local/flume/conf/flume.conf
6. View the streaming data
- hdfs dfs -ls /user/flume/tweets
- hdfs dfs -cat /user/flume/tweets/FlumeData.1431058050787
- hdfs dfs -rm /user/flume/tweets/*.tmp
7. Analyse data using Hive
- wget http://files.cloudera.com/samples/
hive-serdes-1.0-SNAPSHOT.jar
- mv hive-serdes-1.0-SNAPSHOT.jar /usr/local/apache-hive-
1.1.0-bin/lib/
- hive
- Link
Apache NiFi : https://nifi.apache.org
Apache Solr : http://lucene.apache.org/solr/
Twitter App : https://apps.twitter.com
Twitter Dashboard : https://raw.githubusercontent.com/hortonworks
/tutorials/hdp-2.3/assets/nifi-sentiment-analytics/assets/Twitter_Flow.xml
Sqoop Laboratory
13:00 - 17:30
- Sqoop [ SQL-to-Hadoop ]
--> is a straightforward command-line tool with the following capabilities
- Imports individual tables or entire databases to files in HDFS
- Generates Java classes to allow you to interact with your imported data
- Provides the ability to import from SQL databases straight into your Hive data warehouse
- Sqoop Benefit
- Leverages RDBMS metadata to get the column data types
- It is simple to script and uses SQL
- It can be used to handle change data capture by importing daily transactional data to Hadoop
- It uses MapReduce for export and import that enables parallel and efficient data movement
- Sqoop Mode
- Sqoop import: Data moves from RDBMS to Hadoop
- Sqoop export: Data moves from Hadoop to RDBMS
- Use Case
- ETL for Data Warehouse
- ELT
- Data Analysis
- Data Archival
- Data Consolidation
- Move reports to Hadoop
- Hands-On : Loading Data from DBMS to Hadoop HDFS
- MySQL RDS Server on AWS
--> A RDS Server is running on AWS with the following configuration
> database: imc_db
> username: admin
> password: imcinstitute
> addr: imcdb.cmw65obdqfnx.us-west-2.rds.amazonaws.com
- Table in MySQL RDS
- country_tbl : Columns: id, country
- movie_rating : Columns: userid, movieid, rating, timestamp
- Installing DB driver for Sqoop
- cd /var/lib/sqoop
- sudo wget https://www.dropbox.com/s/6zrp5nerrwfixcj/
mysql-connector-java-5.1.23-bin.jar
- Restart Sqoop2 service
- Importing data from MySQL to HDFS
- sudo -u hdfs sqoop import --connect jdbc:mysql://imcdb.cmw65obdqfnx.us-west-
2.rds.amazonaws.com:3306/imc_db --username admin --password
imcinstitute --table movie_rating --target-dir /user/guest1/movietable -m 1
- Importing data from MySQL to Hive Table
- sudo -u hdfs sqoop import --connect
jdbc:mysql://imcdb.cmw65obdqfnx.us-west-
2.rds.amazonaws.com:3306/imc_db --username admin --password
imcinstitute --table country_tbl --hive-import --hive-table
thanachart.country -m 1
- Reviewing data from Hive Table
- Importing data from MySQL to HBase
- sudo -u hdfs sqoop import --connect
jdbc:mysql://imcdb.cmw65obdqfnx.us-west-
2.rds.amazonaws.com:3306/imc_db --username admin --password
imcinstitute --table country_tbl --hbase-table
thanachart.hbase_country --column-family hbase_country_cf --hbase-rowkey id --hbase-create-table -m 1
- Start HBase
- hbase shell
- Viewing Hbase data
- Link
Sqoop Hands-On : http://kundjanasith.com/BigDataSchool/13-06-2016/Hadoop-Handon-June2016.pdf
